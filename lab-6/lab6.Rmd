---
title: 'Lab 6'
author: "Zeren Li"
output: pdf_document
---

## Roadmap

- Test for heteroskedasticity (visualization, rvf, BP, and White test)
- Heteroskedasticity-robust standard error
- Weighted Least Squares & Feasible Generalizable Least Squares

```{r, include=FALSE}
library(AER) # For the testsi
library(wooldridge)
library(tidyverse)
library(GGally)
library(stargazer)
library(car)

# set ggplot theme
theme_set(theme_minimal())
```


## Earning Dataset

Wooldridge's notes on the data set: "Notes:  I remember entering this data set in the late 1980s,  and I am pretty sure it came directly from an introductory econometrics text.  But so far my search has been fruitless.  If anyone runs across this data set, I would appreciate knowing about it."

```{r}
data("saving", package="wooldridge")  #from wooldridge package

# summary statistics
summary(saving)
```

### Fit a bivariate regression

```{r}
m_het <- lm(sav ~ inc, saving)

# get y_hat
y_hat <- predict(m_het)

# residual
u_hat <- matrix(saving$sav - y_hat)

# variance/covariance matrix
v_cov <-   u_hat %*% t(u_hat)
v_cov[1:5,1:5]
```

## Test for Heteroskedasticity

### Visualized Analysis

This graph does not show the classic case of fanning-out along with different levels of the dependent variable

```{r}
df =  data.frame(sav = saving$sav, u_hat = u_hat) 

ggplot(df, aes(x = sav, y = u_hat) ) +
  geom_point()
```

### Regression Diagnostics

The RVF plot does cause me to worry somewhat. We see very little variance along levels of the fitted values until we hit about $1000 and then the variance increases,  declines, and increases again.

```{r}
par(mfrow=c(2,2))
plot(m_het)
```

### Breush-Pagan and White test

### BP test by hand

Recall that our Null hypothesis is that we have homoskedasticity

*BP test 

1. Estimate the model `y ~ x1 + x2 + ... + xk` by OLS, as usual. Obtain the squared OLS residuals, $\hat u^2$ one for each observation.

2. Run the regression $\hat u^2 = \delta_0 + \delta_1 x_1 + \delta_2 x_2 + \dots + \delta_k x_k$. Keep the R-squared from this regrsesion, $R^2_{\hat u^2}$.

3. Form either the $F$ statistic or the $LM$ statistic and compute the p-value (using the $F_{k,n-k-1}$ distribution in the former case and the $\chi_k^2$ distribution in the latter case)

\[
F = \frac{R^2_{\hat u_i^2} / k}{(1 - R^2_{\hat u_i^2}) / (n - k - 1)}; 
LM = n \times R^2_{\hat u_i^2}
\]

```{r}
y <- saving$sav
x <- saving$inc

m_bp <- lm(y ~ x)
squared_residuals <- resid(m_bp) ** 2
m_bp_stage2 <- lm(squared_residuals ~ x)
R_squared <- summary(m_bp_stage2)$r.squared

n <- length(y) ; k <- 1 # k = 1 because we only have 1 independent variable
```

Is this F-statistic large or small?

```{r}
(F_statistic <- (R_squared / k) / ((1 - R_squared) / (n - k - 1)))

plot(density(rf(1000, df1 = k , df2 = n - k - 1)),
     main = "F distribution, df1=k, df2=n-k-1") ; abline(v = F_statistic, col = 'red')
```

Given this F-statistic, what's the p-value?
```{r}
1 - pf(F_statistic, df1 = k, df2 = n - k - 1)
```

How about lm statistic? 

```{r}
(lm_st <- n*(R_squared))
(1 - pchisq(lm_st, df = k))

plot(density(rchisq(1000,1)),
     main = "Chi-squared distribution, df1=k") ; abline(v = lm_st, col = 'red')
```


```{r}
(t_bp <- bptest(m_bp, varformula = ~ x ))

# R use lm statistics
t_bp$statistic
t_bp$p.value
```

The default Breusch-Pagan test is a test for linear forms of heteroskedasticity, e.g. as $\hat y$ goes up, the error variances go up. In this default form, the test does not work well for non-linear forms of heteroskedasticity, such as the hourglass shape we saw before (where error variances got larger as X got more extreme in either direction). The default test also has problems when the errors are not normally distributed 

Part of the reason the White test is more general because it adds a lot of terms to test for more types of heteroskedasticity. For example, adding the squares of regressors helps to detect nonlinearities such as the hourglass shape. In a large data set with many explanatory variables, this may make the test difficult to calculate. Also, the addition of all these terms may make the test less powerful in those situations when a simpler test like the default Breusch-Pagan would be appropriate, i.e. adding a bunch of extraneous terms may make the test less likely to produce a significant result than a less general test would.

([Source](https://www3.nd.edu/~rwilliam/stats2/l25.pdf))

*White test 
(Wooldridge *Introductory Econometrics*, Chpt 8, Testing for heteroskedasticity)*

1. Estimate the model `y ~ x_1 + x_2 + ... + x_k` by OLS, as usual. Obtain the OLS residual $\hat u$ and the fitted values $\hat y$. Compute $\hat u^2$ and $\hat y^2$.

2. Run the regression $\hat u^2 = \delta_0 + \delta_1 \hat y + \delta_2 \hat y^2$.

3. Form either the F or LM statistic and compute the p-value (using the $F_{2,n-3}$ distribution in the former case and the $\chi_2^2$ distribution in the latter case).

```{r}
bptest(m_het, varformula = ~ y_hat + I(y_hat^2))
```

## Robust standard error

Formula for standard error is:
\[
V(\hat \beta_{OLS}|X) = (X'X)^{-1}X'V(\epsilon | X) X(X'X)^{-1}
\]

, which reduces to $\sigma^2 (X'X)^{-1}$ only in the case of homoskedasticity. Also, notice that we call it "sandwich" as the "bread" ($(X'X)^{-1}$) and the "meat" ($X'V(\epsilon|X)X$).




When there's heteroskedasticity, the standard error is as follows:

\begin{align}
V(\hat \beta_{OLS}|X) &= (X'X)^{-1} X'V(\epsilon | X) X(X'X)^{-1} \\
\hat V (\hat \beta_{OLS}|X) &= (X'X)^{-1} X' 
\left(\begin{matrix} 
\hat \epsilon^2_1 & 0 & \dots & 0 \\
0 & \hat \epsilon^2_2 & \dots & 0 \\
\vdots & \vdots & \dots & \vdots \\
0 & 0 & \dots & \hat \epsilon^2_n
\end{matrix}\right) X (X'X)^{-1} \\
\hat V (\hat \beta_{OLS}|X) &= \frac{N}{N-K} (X'X)^{-1} \sum_{i=1}^N \{ X_i X_i' \hat \epsilon_i^2\} (X'X)^{-1} \qquad 
\end{align}


The constant is added since we are estimating the sample variance of the error. 

White HC standard errors (sandwich standard error)

```{r}
vcovHC(m_het, type = "HC") # from package sandwich, loaded by AER
```

Hypothesis test with White standard error (more precisely, with the heteroskedasticity-consistent estimation of the covariance matrix, calculated above). Notice how the coefficients are the same as regular OLS. Only the standard error is **different**.

```{r}
(robust <- coeftest(m_het, vcov = vcovHC(m_het, type = "HC")))
summary(m_het)
```

# Regression comparison

```{r, results= "asis"}
stargazer(m_het, robust, header = F ,
          column.labels = c("ols", "robust se"))
```

\newpage

## Get the robust standard error by hand

From Wooldridge "Introductory" - Heteroskedasticity robust inference after OLS estimation

\[
\hat{Var} (\hat \beta_j) = \frac{\sum_{i=1}^{n} \hat r^2_{ij}\hat u_i^2}{SSR_j^2}
\]

where $\hat r^2_{ij}$ is the ith residual from regressing $x_j$ on all other independent variables, and $SSR_j$ is the sum of squared residuals from this regression

```{r}
m_robust1 <- lm(x ~ 1)
numerator <- sum((resid(m_robust1)**2) * (resid(m_het)**2))
SSR <- sum(resid(m_robust1)**2) ** 2

(var_beta_x <- numerator / SSR)
```
We see that we get exactly the same $\hat Var(\hat \beta_x)$ as output by R.

```{r}
(vcovHC(m_het, type = "HC"))
```

\newpage 

## Weighted Least Squares

```{r}
# specifying weight in lm()
wls1 = lm (sav ~ inc, saving,  weights=(1/inc ))
summary(wls1)

# the weight is used 
wls_bh =  saving %>%
  mutate(weight = 1/(sqrt(inc)),
         hsav = sav*weight,
         hinc = inc*weight ) %>%
  lm(hsav ~ hinc + weight + 0 ,. )

summary(wls_bh)

# multivariate example
m_ols <- lm ( sav ~ inc + size + educ + age + black, saving )
y_hat1 <- predict(m_ols)

# bp test
bptest(m_ols)

# white test
bptest(m_ols, varformula = ~ y_hat1 + I(y_hat1^2))

# WLS
m_wls <- lm ( sav ~ inc + size + educ + age + black  , saving, weights = (1/inc ) )

```


\newpage 

## Feasible Generalizable Least Squares

```{r}
# Run the regression of y on the x's and obtain the residual
wls3 <- lm ( sav ~ inc + size + educ + age + black, saving)
uhat_FGLS <- saving$sav - predict(wls3)

# Take the natural log of the squared residuals
luhat2_FGLS <- log(uhat_FGLS^2)

#  Regress the log of the squared residulas on the independent 
fg <- lm(luhat2_FGLS ~ inc + size + educ + age + black, saving)

# Obtain the fitted values
g_hat <- predict(fg)

# Exponentiate the fitted values
h_hat <- exp(g_hat)

# Take the square root of h_hat
hhat_sqrt <- sqrt(h_hat)

fgls_saving = saving %>%
  # add the weight into data
  mutate(hhat_sqrt = hhat_sqrt,
         weight = 1/hhat_sqrt,
         ) %>%
  mutate_at(.vars = c("sav","inc","size","educ","age","black"),
            funs(./hhat_sqrt),
            ) 

m_fgls_bh <- lm(sav ~ inc + size + educ + age + black + weight + 0, fgls_saving)

summary(m_fgls_bh)

# double check using built-in function
m_fgls <- lm(sav ~ inc + size + educ + age + black , saving, weights = 1/h_hat)
summary(m_fgls)


```

### Regression comparasion

```{r, results= "asis"}
stargazer( m_ols ,m_wls,  m_fgls,
           type = "latex",
           header = F,
           column.labels = c("ols", "wls", "fgls")
           ) 
```


