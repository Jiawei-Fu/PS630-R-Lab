---
title: "R Lab 2"
author: "Zeren Li"
date: "9/2/2019"
output:
   pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls()) # remove all object
```

## Roadmap

- RStudio server, R Style
- `tidyverse` 
- T-test

## Advanced programming for data science 

### `tidyverse`

"The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures."

The core tidyverse includes the packages that you're likely to use in everyday data analyses. As of tidyverse 1.2.0, the following packages are included in the core tidyverse:

- `magrittr`: offers a set of operators which make your code more readable
- `dplyr`: a grammar of data manipulation
- `ggplot2`:a system for declaratively creating graphics
- `tidyr`: a set of functions that help you get to tidy data
- `readr`: provides a fast and friendly way to read rectangular data 
- `purrr`: enhances Râ€™s functional programming (FP) toolkit by providing a complete and consistent set of tools for working with functions and vectors
- `tibble`: a modern re-imagining of the data frame
- `stringr`: a cohesive set of functions designed to make working with strings as easy as possible
- `forcats`: provides a suite of useful tools that solve common problems with factors

```{r,include = FALSE}
# use the following code to install tidyverse
#install.packages("tidyverse") 
#install.packages("haven") 
#install.packages("xtable") 

library(haven)
library(tidyverse)
library(xtable)
```

### `magrittr`

The `magrittr` does the following:

 - structuring sequences of data operations left-to-right (as opposed to from the inside and out)
 - avoiding nested function calls
 - minimizing the need for local variables and function definitions
 - making it easy to add steps anywhere in the sequence of operations

You can think about the following sequence of actions - find key, unlock car, start car, drive to school, park.

Expressed as a set of nested functions in R pseudocode this would look like:

```{r eval=FALSE}
park(drive(start_car(find("keys")), to="campus"))
```

Writing it out using pipes give it a more natural (and easier to read) structure:

```{r eval=FALSE}
find("keys") %>%
    start_car() %>%
    drive(to = "campus") %>%
    park()
```

## Approaches

Basic piping
`x %>% f` is equivalent to `f(x)`
`x %>% f(y)` is equivalent to `f(x, y)`
`x %>% f %>% g` %>% h is equivalent to `h(g(f(x)))`

The argument placeholder

`x %>% f(y, .)` is equivalent to `f(y, x)`
`x %>% f(y, z = .)` is equivalent to `f(y, z = x)`


## `dplyr` - A Grammar of Data Manipulation

`dplyr` is based on the concepts of functions as verbs that manipulate data frames.

Single data frame functions / verbs:

- * `filter()`: filter rows by condition(s)
- * `slice()`: filter rows using index(es)
- * `select()`: select columns by name
- * `rename()`: rename variables
- * `arrange()`: reorder rows
- * `mutate()`: add new variables
- * `distinct()`: filter for unique rows
- * `sample_n()` / `sample_frac()`: randomly sample rows
- * `summarise()`: reduce variables to values
- * ... (many more)

## CEO Data Analysis

###  Read CEO data

```{r}
# set working directory
setwd("~/PS630-R-Lab/lab-2") # change to your own working directory
# read dta (Stata)
ceo <- read_dta("./CEOSAL2.DTA") # read CEO dataset using haven
```

### `filter()` -  filter by condition(s)

```{r}
ceo %>% filter(age > 50)

ceo %>% filter(age == 50, salary > 1000 )

```

### `slice()` - first 10 columns

```{r}
ceo %>% slice(1:10)
```
### `slice()` - last 5 columns

```{r}
ceo %>% slice((n()-4):n())
```


### `select()` - select columns

```{r}
ceo %>% select(salary, profmarg)
```

### `select()` - exclude columns

```{r}
ceo %>% select(-salary, -profmarg)
ceo %>% select(-c(salary, profmarg))
```

### `select()` - ranges

```{r}
ceo %>% select(salary:college)
```

### `select()` - excluse ranges

```{r}
ceo %>% select(-c(salary:college))
```

## `rename()` - change column names

```{r}
names(ceo)
ceo_new <- ceo %>% rename(profit_margin = profmarg)
names(ceo_new)
```

## `arrange()` - sort data

```{r}
ceo %>% 
  # filter if age is larger than 50
  filter(age > 50) %>%
  # sort by age and then salary
  arrange(age,salary)  

ceo %>% 
  # filter if age is larger than 50
  filter(age > 50) %>%
  # sort by age (descend)
  arrange(desc(age),salary)
```

### `mutate()` - modify columns

```{r}
ceo %>% 
  # add a new variable salary_l: log(salary)
  mutate(salary_l = log(salary)) %>%
  # select salary and salary_l
  select(salary, salary_l, lsalary)
```

### `distinct()` - find unique rows

```{r}
ceo %>% 
  select(age, ceoten) %>% 
  distinct() %>% 
  arrange(age, ceoten)
```

### `sample_n()` sampling rows

```{r}
ceo %>% sample_n(100)
```

## `summarise()` - summarize data

```{r}
ceo %>% 
  # provide summary statistic: # of obs, min, max
  summarize(n = n(), 
            mean = mean(salary,na.rm = T),
            min = min(salary,na.rm = T), 
            max = max(salary,na.rm = T))
```

### Tabulate Data by `grad`

```{r}
# creat your own contingency table
ceo_tab = ceo %>% 
  # define subgroups
  group_by(grad) %>% 
  # provide summary statistic: # of obs, min, max
  summarize(n = n(), 
            mean = mean(salary,na.rm = T),
            sd = var(salary,na.rm = T) %>% sqrt(.),
            min = min(salary,na.rm = T), 
            max = max(salary,na.rm = T)) 
ceo_tab 
```

### Using `xtable()` to export 

```{r, results =  "asis"}
xtable(ceo_tab)
```


## T-test

### Tabulate your data

```{r}
# creat your own table
tab <- ceo %>% 
  # define subgroups
  group_by(grad) %>% 
  # provide summary statistic: # of obs, min, max
  summarize(n = n(), 
            mean = mean(profmarg,na.rm = T),
            sd = var(profmarg,na.rm = T) %>% sqrt(.),
            min = min(profmarg,na.rm = T), 
            max = max(profmarg,na.rm = T))
tab 
```

### Construct T-statistic

$$
t =  \frac{\bar X_t - \bar X_c}{\hat \sigma_{\bar{X_t}- \bar{X_c}}} \\ 
$$
where:

$$
\hat \sigma_{\bar{X_t}- \bar{X_c}} = \sqrt{\frac{\hat \sigma_{\bar{X_t}}}{n_t} + 
\frac{\hat \sigma_{\bar{X_c}}}{n_c}
}  \\
$$


```{r}
# number of observations
n_c <- tab$n[1]
n_t <- tab$n[2]

# mean 
mean_c <- tab$mean[1]
mean_t <- tab$mean[2]

# standard deviation
sd_c <- tab$sd[1]
sd_t <- tab$sd[2]

# compute sigma
signma_tc <- sqrt(sd_c^2/n_c + sd_t^2/n_t)

# compute t-statistic
t_test <- (mean_t - mean_c)/ signma_tc

t_test
```

### The degrees of freedom 

R uses Welch DoF, which is estimated as follows:

$$
\nu_{_W} = \frac{\left(\frac{s_t^2}{n_t}+\frac{s_c^2}{n_c}\right)^2}{\frac{s_t^4}{n_t^2\nu_t}+\frac{s_c^4}{n_c^2\nu_c}} 
$$

```{r}
# numerator
num = (sd_t^2/n_t + sd_c^2/n_c)^2

# denominator
den = sd_t^4/( (n_t^2) * (n_t - 1)) + sd_c^4/( (n_c^2) * (n_c - 1))

# degrees of freedom
dof = num/den
dof
```

# Confidence Interval

$$
CI = (\bar X - t^* * \hat \sigma_{\bar{X_t}- \bar{X_c}}, \bar X + t^* * \hat \sigma_{\bar{X_t}- \bar{X_c}})
$$

where $\sigma_{\bar{X_t}- \bar{X_c}}$ is the estimated standard error, $t^*$ is the critical value of $t$ for the desired level of confidence.


```{r}
mean_dif <-  mean_c - mean_t
mean_dif

critical_t <- qt(0.05/2, dof)

mean_dif - critical_t*signma_tc
mean_dif + critical_t*signma_tc

```

### P value

$$
|T| > t_{1-\alpha/2,\nu}
$$

```{r}
2*pt(-abs(t_test),df = dof) # pt() is the distribution function of t Distribution
```

### Visualizing P value

```{r echo=FALSE}
x <- seq(-4,4,length=1000)
y <- dt(x, dof)
df = data.frame(x, y)

df_left <- rbind(c(-abs(t_test),0), subset(df, x < -abs(t_test)))
df_right <- rbind(c(abs(t_test),0), subset(df, x > abs(t_test)))

ggplot(df) +
  geom_line(aes(x= x, y = y)) + 
  geom_polygon(data = df_left, aes(x=x, y=y, fill="red")) +
  geom_polygon(data = df_right, aes(x=x, y=y, fill="red")) +
  guides(fill="none") +
  geom_vline(xintercept = abs(critical_t), color = "blue") +
  geom_vline(xintercept = -abs(critical_t), color = "blue") +
  xlab("t") +
  ylab("")
  
```



### T-test using `t.test`

```{r}
 t.test( profmarg ~ grad  , data = ceo )
```
 